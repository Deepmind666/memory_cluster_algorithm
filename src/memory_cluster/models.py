from __future__ import annotations

from dataclasses import asdict, dataclass, field
from datetime import datetime, timezone
from typing import Any


def utc_now_iso() -> str:
    """Return current UTC timestamp in ISO 8601 format."""
    return datetime.now(timezone.utc).isoformat()


@dataclass
class MemoryFragment:
    """Smallest memory unit generated by an agent or tool execution."""

    id: str
    agent_id: str
    timestamp: str
    content: str
    type: str
    tags: dict[str, Any] = field(default_factory=dict)
    provenance: list[str] = field(default_factory=list)
    meta: dict[str, Any] = field(default_factory=dict)
    version: int = 1

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "MemoryFragment":
        return cls(
            id=str(data["id"]),
            agent_id=str(data["agent_id"]),
            timestamp=str(data.get("timestamp") or utc_now_iso()),
            content=str(data.get("content") or ""),
            type=str(data.get("type") or "dialog"),
            tags=dict(data.get("tags") or {}),
            provenance=list(data.get("provenance") or []),
            meta=dict(data.get("meta") or {}),
            version=int(data.get("version") or 1),
        )

    def to_dict(self) -> dict[str, Any]:
        return asdict(self)


@dataclass
class ConflictRecord:
    """Structured conflict marker kept in compressed cluster outputs."""

    slot: str
    values: list[str]
    evidences: list[str]
    last_seen: str
    priority: float = 0.0
    dominant_value: str = ""
    transition_count: int = 0

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "ConflictRecord":
        return cls(
            slot=str(data["slot"]),
            values=[str(v) for v in (data.get("values") or [])],
            evidences=[str(v) for v in (data.get("evidences") or [])],
            last_seen=str(data.get("last_seen") or utc_now_iso()),
            priority=float(data.get("priority") or 0.0),
            dominant_value=str(data.get("dominant_value") or ""),
            transition_count=int(data.get("transition_count") or 0),
        )

    def to_dict(self) -> dict[str, Any]:
        return asdict(self)


@dataclass
class MemoryCluster:
    """Cluster object with centroid, summary, conflicts, and provenance pointers."""

    cluster_id: str
    centroid: list[float]
    fragment_ids: list[str] = field(default_factory=list)
    source_distribution: dict[str, int] = field(default_factory=dict)
    tags: dict[str, Any] = field(default_factory=dict)
    consensus: dict[str, Any] = field(default_factory=dict)
    conflicts: list[ConflictRecord] = field(default_factory=list)
    conflict_graph: dict[str, Any] = field(default_factory=dict)
    split_groups: list[dict[str, Any]] = field(default_factory=list)
    parent_cluster_id: str | None = None
    child_cluster_ids: list[str] = field(default_factory=list)
    level: int = 1
    summary: str = ""
    backrefs: list[str] = field(default_factory=list)
    last_updated: str = field(default_factory=utc_now_iso)
    version: int = 1

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "MemoryCluster":
        conflicts = [
            ConflictRecord.from_dict(item) if not isinstance(item, ConflictRecord) else item
            for item in (data.get("conflicts") or [])
        ]
        return cls(
            cluster_id=str(data["cluster_id"]),
            centroid=[float(x) for x in (data.get("centroid") or [])],
            fragment_ids=[str(x) for x in (data.get("fragment_ids") or [])],
            source_distribution=dict(data.get("source_distribution") or {}),
            tags=dict(data.get("tags") or {}),
            consensus=dict(data.get("consensus") or {}),
            conflicts=conflicts,
            conflict_graph=dict(data.get("conflict_graph") or {}),
            split_groups=list(data.get("split_groups") or []),
            parent_cluster_id=(str(data["parent_cluster_id"]) if data.get("parent_cluster_id") else None),
            child_cluster_ids=[str(x) for x in (data.get("child_cluster_ids") or [])],
            level=int(data.get("level") or 1),
            summary=str(data.get("summary") or ""),
            backrefs=[str(x) for x in (data.get("backrefs") or [])],
            last_updated=str(data.get("last_updated") or utc_now_iso()),
            version=int(data.get("version") or 1),
        )

    def to_dict(self) -> dict[str, Any]:
        payload = asdict(self)
        payload["conflicts"] = [item.to_dict() for item in self.conflicts]
        return payload


@dataclass
class PreferenceConfig:
    """Preference vector controlling retention and compression behavior."""

    category_strength: dict[str, str] = field(default_factory=dict)
    source_weight: dict[str, float] = field(default_factory=dict)
    stale_after_hours: int = 72
    detail_budget: dict[str, int] = field(
        default_factory=lambda: {"strong": 700, "weak": 350, "discardable": 120}
    )
    keep_conflicts: bool = True
    strict_conflict_split: bool = False
    source_promote_threshold: float = 1.5
    source_demote_threshold: float = 0.8
    semantic_dedup_threshold: float = 0.88
    enable_l2_clusters: bool = False
    l2_min_children: int = 2
    enable_conflict_graph: bool = False
    enable_adaptive_budget: bool = False
    arb_conflict_weight: float = 0.45
    arb_entropy_weight: float = 0.25
    arb_stale_penalty: float = 0.35
    arb_min_scale: float = 0.7
    arb_max_scale: float = 1.6
    enable_dual_merge_guard: bool = False
    merge_conflict_compat_threshold: float = 0.55
    enable_merge_upper_bound_prune: bool = False
    merge_prune_dims: int = 48
    enable_merge_candidate_filter: bool = False
    merge_candidate_bucket_dims: int = 10
    merge_candidate_max_neighbors: int = 16
    enable_merge_ann_candidates: bool = False
    merge_ann_num_tables: int = 4
    merge_ann_bits_per_table: int = 8
    merge_ann_probe_radius: int = 0
    merge_ann_max_neighbors: int = 24
    merge_ann_score_dims: int = 32
    hard_keep_tags: list[str] = field(default_factory=list)
    protected_path_prefixes: list[str] = field(default_factory=list)
    protected_scopes: list[str] = field(default_factory=lambda: ["global_task", "current_task"])

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "PreferenceConfig":
        return cls(
            category_strength=dict(data.get("category_strength") or {}),
            source_weight={k: float(v) for k, v in (data.get("source_weight") or {}).items()},
            stale_after_hours=int(data.get("stale_after_hours") or 72),
            detail_budget=dict(data.get("detail_budget") or {"strong": 700, "weak": 350, "discardable": 120}),
            keep_conflicts=bool(data.get("keep_conflicts", True)),
            strict_conflict_split=bool(data.get("strict_conflict_split", False)),
            source_promote_threshold=float(data.get("source_promote_threshold", 1.5)),
            source_demote_threshold=float(data.get("source_demote_threshold", 0.8)),
            semantic_dedup_threshold=float(data.get("semantic_dedup_threshold", 0.88)),
            enable_l2_clusters=bool(data.get("enable_l2_clusters", False)),
            l2_min_children=max(2, int(data.get("l2_min_children", 2))),
            enable_conflict_graph=bool(data.get("enable_conflict_graph", False)),
            enable_adaptive_budget=bool(data.get("enable_adaptive_budget", False)),
            arb_conflict_weight=float(data.get("arb_conflict_weight", 0.45)),
            arb_entropy_weight=float(data.get("arb_entropy_weight", 0.25)),
            arb_stale_penalty=float(data.get("arb_stale_penalty", 0.35)),
            arb_min_scale=float(data.get("arb_min_scale", 0.7)),
            arb_max_scale=float(data.get("arb_max_scale", 1.6)),
            enable_dual_merge_guard=bool(data.get("enable_dual_merge_guard", False)),
            merge_conflict_compat_threshold=float(data.get("merge_conflict_compat_threshold", 0.55)),
            enable_merge_upper_bound_prune=bool(data.get("enable_merge_upper_bound_prune", False)),
            merge_prune_dims=max(0, int(data.get("merge_prune_dims", 48))),
            enable_merge_candidate_filter=bool(data.get("enable_merge_candidate_filter", False)),
            merge_candidate_bucket_dims=max(1, int(data.get("merge_candidate_bucket_dims", 10))),
            merge_candidate_max_neighbors=max(1, int(data.get("merge_candidate_max_neighbors", 16))),
            enable_merge_ann_candidates=bool(data.get("enable_merge_ann_candidates", False)),
            merge_ann_num_tables=max(1, int(data.get("merge_ann_num_tables", 4))),
            merge_ann_bits_per_table=max(1, int(data.get("merge_ann_bits_per_table", 8))),
            merge_ann_probe_radius=max(0, min(1, int(data.get("merge_ann_probe_radius", 0)))),
            merge_ann_max_neighbors=max(1, int(data.get("merge_ann_max_neighbors", 24))),
            merge_ann_score_dims=max(1, int(data.get("merge_ann_score_dims", 32))),
            hard_keep_tags=[str(x) for x in (data.get("hard_keep_tags") or [])],
            protected_path_prefixes=[str(x) for x in (data.get("protected_path_prefixes") or [])],
            protected_scopes=[str(x) for x in (data.get("protected_scopes") or ["global_task", "current_task"])],
        )

    def to_dict(self) -> dict[str, Any]:
        return asdict(self)


@dataclass
class ClusterBuildResult:
    """Output container after running full ingest -> cluster -> compress pipeline."""

    fragments: list[MemoryFragment]
    clusters: list[MemoryCluster]
    metrics: dict[str, Any]
    build_timestamp: str = field(default_factory=utc_now_iso)

    def to_dict(self) -> dict[str, Any]:
        return {
            "build_timestamp": self.build_timestamp,
            "fragments": [item.to_dict() for item in self.fragments],
            "clusters": [item.to_dict() for item in self.clusters],
            "metrics": self.metrics,
        }
